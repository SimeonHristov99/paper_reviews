########################################################################################################################
# 2001
########################################################################################################################

Statistical Modeling: The Two Cultures
    What did the authors try to accomplish?
        Proposes the use of algorithmic models (decision trees, neural nets) to reach conclusions from data, as opposed to assume that data is generated by a stochastic data model (regressions).
    What were the key elements of the approach?
        Very interesting way of writing! Real-world experiences shared that demonstrate that using basic models like regressions can lead to wrong conclusions.
        The critique is based on a lot of practical examples.
        Alternatives are discussed: decision trees, svm, neural nets.
    What can you use yourself?
        Those should be takes as life advice :)
            (a) Focus on finding a good solution—that’s what consultants get paid for.
            (b) Live with the data before you plunge into modeling.
            (c) Search for a model that gives a good solution, either algorithmic or data.
            (d) Predictive accuracy on test sets is the criterion for how good the model is.
            (e) Computers are an indispensable partner.
        There are a lot of gems in this paper. Time should be spent to read it carefully.
    What other references do you want to follow?
        None, since this is more of an article.


########################################################################################################################
# 2011
########################################################################################################################

Comparison of Reasoners for large Ontologies in the OWL 2 EL Profile
    What did the authors try to accomplish?
        List characteristics by which to compare a set of reasoners for large ontologies.
    What were the key elements of the approach?
        They list and describe the characteristics - what they mean, why they use them, etc.
        They describe the ontologies they used for comparison.
        They describe the reasoners they tested.
        Characterization dimensions: reasoning characteristics, practical usability, performance indicators.
        Reasoners: CB, CEL, FaCT++, HermiT, Pellet, RacerPro, Snorocket, TrOWL.
        Ontologies: GO, NCI, SNOMED CT.
    What can you use yourself?
        The main take from this papers is section 5 (Categorization of Reasoners) in which they describe the benchmarking and categorization procedure.
        It shows how one would go about choosing a reasoner for a real world application - the characteristics they would use, how they would compare the reasoners, etc.
    What other references do you want to follow?
        the references referring to the reasoners and logic rules

########################################################################################################################
# 2014
########################################################################################################################

Tidy Data 
    What did the authors try to accomplish?
        Very important classic and foundational table. Introduces the principles of data tidying.
    What were the key elements of the approach?
        Lots of examples on how to tidy a dataset are given. Also a case study at the end.
    What can you use yourself?
        structure of tidy datasets: each variable is a column, each observation is a row, and each type of observational unit is a table
    What other references do you want to follow?
        This is more of an article, rather than a research paper. It is important by itself.

########################################################################################################################
# 2011
########################################################################################################################

Understanding Deep Neural Networks with Rectified Linear Units
    What did the authors try to accomplish?
        Present an algorithm to train deep neural networks that have ReLU activation functions. Basically, present an algorithm that trains faster.
    What were the key elements of the approach?
        A lot of math. Use of theory of zonotopes from polyhedral theory.
    What can you use yourself?
        Hard to read paper.
    What other references do you want to follow?
        None, as I'm not really interested in this topic.


########################################################################################################################
# 2020
########################################################################################################################

AI-bank of the future: Can banks meet the AI challenge?
    What did the authors try to accomplish?
        Present a methodology / blueprint / changes that need to happen in order for banks to integrate AI at scale in order to remain relevant.
    What were the key elements of the approach?
        They list the challenges and benefits of using AI, going cloud, etc. Show how to overcome the challenges.
        They describe in detail what the changes will bring and how to think about them.
    What can you use yourself?
        The main take is the Exhibit 6 in which they show how a bank would need to make internal changes and also section 4 (How can banks transform to become AI-first?).
        There they describe what each layer of changes is and how it will affect the bank.
        It doesn't have a lot of case studies. It is more broad and abstract rather than specific.
    What other references do you want to follow?
        No references listed, but one could research some of the ideas in the article, for example, using facial expression evaluation in order to grant loan / approve transaction.

Banks banking on AI
    What did the authors try to accomplish?
        Outline the current trends in banking not only with abstract/general statements but also with several short case-studies.
    What were the key elements of the approach?
        They describe what the problems in AI for banks are.
        They also describe in what areas AI should be applied (customer service, customer engagement, chatbots, fraud management, etc).
        They case-studies really help to see the current practical aspects of AI in banks.
    What can you use yourself?
        It seems like chatbots (and thus NLP) are the most wide spread form of AI in the banking sector.
        The area is which AI is applied as well as the case-studies can be used to generate ideas.
    What other references do you want to follow?
        None - this is more of an article, rather than a scientific paper.

########################################################################################################################
# 2021
########################################################################################################################

Advances and Open Problems in Federated Learning
    What did the authors try to accomplish?
        In detail explain what the current state of AI related to federated learning is and what problems are to be solved.
    What were the key elements of the approach?
        State the current situation + State open problems.
        Very detailed.
    What can you use yourself?
        In case one is curious about federated learning and would like to do research in that field, this paper is a great start.
        It has a ton of references and gives examples of datasets on which to benchmark models as well as software (Tensorflow Federated, etc).
    What other references do you want to follow?
        Since, I'm not particularly interested in the topic, I won't list any here.

########################################################################################################################
# 2022
########################################################################################################################

Pen & Paper Exercises in Machine Learning
    What did the authors try to accomplish?
        This is a useful collection of mathematical exercises and solutions related to the field of machine learning.
        Topics covered: Linear Algebra, Optimization, Directed Graphical Models, Undirected Graphical Models, Expressive Power of Graphical Models, Factor Graphs and Message Passing, Inference for Hidden Markov Models, Model-Based Learning, Sampling and Monte Carlo Integration, Variational Inference.
    What were the key elements of the approach?
        - detailed solutions
        - various topics covered
    What can you use yourself?
        - could be used as a refresher or all the covered topics
        - could be used for a flashcard system where you are given the question and you have to write the solution
    What other references do you want to follow?
        - (book) Monte Carlo theory, methods and examples: https://artowen.su.domains/mc/
        - (book) Pattern Recognition and Machine Learning by Christopher M. Bishop: https://link.springer.com/book/9780387310732
        - Applications of Kalman Filtering in Aerospace 1960 to the Present
        - Fast and robust fixed-point algorithms for independent component analysis

How to play any mental game or A Completeness Theorem for Protocols with Honest Majority
    What did the authors try to accomplish?
        Lengthy paper in which the authors lay out a protocol for simulating the trusted party of an ideal game. That is, if more than half of the players follow the protocol, whatever a player (or a set of players of size less than n/2) knows at any step of the game, he would have also known in an idea execution of the game with a trusted party.
    What were the key elements of the approach?
        Related to cryptography and game theory.
        The knowledge constraints are satisfied in a computational complexity sense. Namely, any player in order to compute anything more than his due share of current state, should perform an exponential-time computation.
    What can you use yourself?
        This is mainly a paper to be used for source of ideas. Not as related to AI.
    What other references do you want to follow?
        None, since not directly related to AI.

Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks
    What did the authors try to accomplish?
        They successfully fool a multi-class image classification neural network for all labels in an image by misaligning patches in the feature map, produced by the model.
    What were the key elements of the approach?
        The perturbations are imperceivable.
        They use a new loss function.
        They use a generative model.
    What can you use yourself?
        The concept of using generative models to fool object classification models is new and provides room for research.
    What other references do you want to follow?
        1, 2, 5, 7, 12, 45

Text2Light: Zero-Shot Text-Driven HDR Panorama Generation
    What did the authors try to accomplish?
        Introduce a model that can generate 4K panoramic images without being trained on image-text pairs.
    What were the key elements of the approach?
        - use CLIP: zero-shot text-to-image generation and manipulation without paired data.
        - have two stages: first generate a low resolution image and then scale it up using inverse tone mapping.
        - use two codebooks
    What can you use yourself?
        - the algorithm is very detailed and there are a lot of helpful pictures
        - if writing a paper related to GANs and high quality image generation, this paper will be inspirational
    What other references do you want to follow?
        inverse tone mapping, avatarCLIP, Text2Human, COCO-GAN, ExpandNet, GLIDE, StyleCLIP, Zero-Shot Text-to-Image Generation, Improved techniques for training gans, Deep learning for HDR imaging: state-of-the-art and future trends, CLIP-GEN, The unreasonable effectiveness of deep features as a perceptual metric.

Diabetic foot ulcers monitoring by employing super resolution and noise reduction deep learning techniques
    What did the authors try to accomplish?
        Improve the performance of models that segment diabetic foot ulcers.
    What were the key elements of the approach?
        They combined noise reduction and super-resolution. They also modified the loss function by including additional penalties.
    What can you use yourself?
        Interesting paper if concerned with semantic segmentation of medical images.
    What other references do you want to follow?
        3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 17, 18, 19, 20, 24, 26, 28, 32, 33, 36

ESTA: An Esports Trajectory and Action Dataset
    What did the authors try to accomplish?
        They created an open-source Python package that can collect data about and visualize CSGO matches using this data.
        They named the dataset ESTA and it is the most granular dataset for CSGO data.
        They also benchmarked several models on the task of "win probability" predictions. It seems that MLP was the best.
    What were the key elements of the approach?
        They used data from online games.
        Apart from log loss they also used expected calibration error (ECE) which is a popular metric for win probability models.
    What can you use yourself?
        For research and project ideas the most important section is 6 (Concluding Remarks) as they show how this dataset could be used for other purposes, such as:
            - Trajectory prediction
            - online learning
            - temporal and spatial projects
            - real-world data analysis
    What other references do you want to follow?
        Research more about online learning. Also: 1, 2, 3, 8, 10, 12, 17
